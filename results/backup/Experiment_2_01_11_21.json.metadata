{
    "creation_time": 1635786240.4304006,
    "creation_time_human": "2021-11-01 18:04:00",
    "time_delta": 292.6226415634155,
    "time_delta_human": "4 minutes and 52 seconds",
    "file_dump_time": 0.004000663757324219,
    "file_dump_time_human": "0 seconds",
    "file_dump_size": 62147,
    "file_dump_size_human": "62.1 kB",
    "load_kwargs": {},
    "dump_kwargs": {},
    "function_name": "experiment",
    "function_file": "<ipython-input-6-5446eb010940>:64",
    "args_to_ignore": [],
    "source": "@Cache(\n    cache_path=\"results/{experiment_name}.json\"\n)\ndef experiment(path, experiment_name, experiment_setup):\n    path_str = \"data/victims_of_fall_V1.csv\"\n    df = pd.read_csv(path_str)\n\n    dataset_list = get_dataset(df, experiment_setup[\"dataset_variants\"], eval(experiment_setup[\"only_BMI\"]), eval(experiment_setup[\"floor_labels\"]), eval(experiment_setup[\"binarize\"]))\n\n    all_performance = []\n    for dataset in dataset_list:\n        dataset_variant = dataset[\"dataset_variant\"]\n        print(dataset_variant)\n\n        X = dataset[\"X\"].to_numpy()\n        y = dataset[\"y\"].to_numpy().ravel()\n\n        if eval(experiment_setup[\"use_special_mask\"]) == True and dataset_variant == \"complete\":\n            mask = [0,0,0,0,0,0,1,1,1,1,1,0,2,2,2,2,2,0,3,3,3,3,3,0,4,4,4,4,4]\n        else:\n            if eval(experiment_setup[\"only_BMI\"]) == True:\n                mask = np.arange(X.shape[1]) > 2\n            else:\n                mask = np.arange(X.shape[1]) > 4\n\n        pipe_steps = []\n        if \"reduce_dim\" in experiment_setup.keys():\n            reduce_dim = eval(experiment_setup[\"reduce_dim\"])\n            pipe_steps.append((\"reduce_dim\", reduce_dim[0]))\n\n        if \"scaler\" in experiment_setup.keys():\n            scaler = eval(experiment_setup[\"scaler\"])\n            pipe_steps.append((\"scaler\", scaler[0]))\n            \n        if \"reduce_dim_2\" in experiment_setup.keys():\n            reduce_dim = eval(experiment_setup[\"reduce_dim_2\"])\n            pipe_steps.append((\"reduce_dim_2\", reduce_dim[0]))\n\n        if \"regressor\" in experiment_setup.keys():\n            estimator = eval(experiment_setup[\"regressor\"])\n            pipe_steps.append((\"regressor\", estimator[0]))\n        elif \"clf\" in experiment_setup.keys():\n            estimator = eval(experiment_setup[\"clf\"])\n            pipe_steps.append((\"clf\", estimator[0]))\n\n        if  \"params\" in experiment_setup.keys():\n            params = experiment_setup[\"params\"]\n\n        if \"n_split_inner_kcv\" in experiment_setup.keys():\n            inner_skf = StratifiedKFold(n_splits=experiment_setup[\"n_split_inner_kcv\"], shuffle = True, random_state=42)\n\n        if \"n_split_outer_kcv\" in experiment_setup.keys():\n            outer_skf = StratifiedKFold(n_splits=experiment_setup[\"n_split_outer_kcv\"], shuffle = True, random_state=42)\n\n        parsed_params = eval_params(params, X, mask)\n        metrics = eval(experiment_setup[\"metrics\"])\n\n        pipe = Pipeline(pipe_steps)\n\n        if \"n_split_inner_kcv\" in experiment_setup.keys():\n            if \"hp_optimizer\" not in experiment_setup.keys():\n                hp_search = GridSearchCV(pipe, parsed_params, cv=inner_skf, verbose=1, scoring = metrics[0], return_train_score=True).fit(X,y)\n            else:\n                hp_search = RandomizedSearchCV(pipe, parsed_params, n_iter = 100, cv=inner_skf, verbose=1, scoring = metrics[0], return_train_score=True).fit(X,y)\n\n            cv_dic = cross_validate(hp_search, X, y, cv=outer_skf, scoring=metrics[0], return_estimator=True, return_train_score=True)\n            best_params_cv = [estimator.best_params_ for estimator in cv_dic[\"estimator\"]]\n\n            score = {\n                \"dataset_variant\": dataset_variant,\n                \"experiment_setup\": experiment_setup,\n                \"best_params\": str(best_params_cv),\n                \"mean_test_score\": [np.mean(cv_dic[f\"test_{metric}\"]) for metric in metrics],\n                \"mean_train_score\": [np.mean(cv_dic[f\"train_{metric}\"]) for metric in metrics],\n                \"cv_results\" : str(cv_dic)\n            }\n            \n        else:\n            if \"hp_optimizer\" not in experiment_setup.keys():\n                hp_search = GridSearchCV(pipe, parsed_params, cv=outer_skf, verbose=1, scoring = metrics[0], return_train_score=True).fit(X,y)\n            else:\n                hp_search = RandomizedSearchCV(pipe, parsed_params, n_iter = 100, cv=outer_skf, verbose=1, scoring = metrics[0], return_train_score=True).fit(X,y)\n\n            best_params_cv = hp_search.best_params_\n\n            score = {\n                \"dataset_variant\": dataset_variant,\n                \"experiment_setup\": experiment_setup,\n                \"best_params\": str(best_params_cv),\n                \"mean_test_score\": hp_search.best_score_,\n                \"cv_results\" : str(hp_search.cv_results_)\n            }\n\n        all_performance.append(score)\n                \n    return all_performance\n",
    "backend_metadata": {},
    "parameters": {
        "path": "./results/",
        "experiment_name": "Experiment_2_01_11_21",
        "experiment_setup": {
            "dataset_variants": [
                "complete",
                "only_binary",
                "only_totals"
            ],
            "only_BMI": "True",
            "use_special_mask": "False",
            "reduce_dim": "[PCA(), TruncatedSVD()]",
            "regressor": "[GradientBoostingRegressor(max_depth = 5, n_estimators = 100, learning_rate = 0.1, criterion = 'mse')]",
            "floor_labels": "True",
            "params": [
                {
                    "reduce_dim": "[PCA(), TruncatedSVD()]",
                    "reduce_dim__n_components": "np.arange(1, X.shape[1])"
                }
            ],
            "metrics": "['neg_root_mean_squared_error']",
            "binarize": "False",
            "n_split_outer_kcv": 10
        }
    }
}